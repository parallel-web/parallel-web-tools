{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Spark Streaming Enrichment with Parallel\n\nThis notebook demonstrates real-time data enrichment using Spark Structured Streaming and the Parallel API.\n\n## Prerequisites\n\n```bash\npip install parallel-web-tools[spark]\nexport PARALLEL_API_KEY=\"your-api-key\"\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from parallel_web_tools.integrations.spark import enrich_streaming_batch\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"StreamingEnrichmentDemo\").getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Streaming Source\n",
    "\n",
    "We'll use a rate source to simulate streaming data. In production, this would be Kafka, Kinesis, or file-based sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate streaming data - generates 1 row per second\n",
    "stream_df = (\n",
    "    spark.readStream.format(\"rate\")\n",
    "    .option(\"rowsPerSecond\", 1)\n",
    "    .load()\n",
    "    .selectExpr(\n",
    "        \"value % 5 as company_id\",\n",
    "        \"CASE value % 5 \"\n",
    "        \"  WHEN 0 THEN 'Google' \"\n",
    "        \"  WHEN 1 THEN 'Microsoft' \"\n",
    "        \"  WHEN 2 THEN 'Apple' \"\n",
    "        \"  WHEN 3 THEN 'Amazon' \"\n",
    "        \"  ELSE 'Tesla' \"\n",
    "        \"END as company_name\",\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Streaming schema:\")\n",
    "stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Batch Processing Function\n",
    "\n",
    "The `enrich_streaming_batch` function enriches each micro-batch using the Parallel Task Group API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_df, batch_id):\n",
    "    \"\"\"Process each micro-batch with enrichment.\"\"\"\n",
    "    if batch_df.count() == 0:\n",
    "        return\n",
    "\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"Processing batch {batch_id} ({batch_df.count()} rows)\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Enrich the batch\n",
    "    enriched_df = enrich_streaming_batch(\n",
    "        batch_df,\n",
    "        input_columns={\"company_name\": \"company_name\"},\n",
    "        output_columns=[\"CEO name\", \"Stock ticker symbol\"],\n",
    "        processor=\"lite-fast\",\n",
    "        timeout=120,\n",
    "    )\n",
    "\n",
    "    # Display results (in production, write to a sink)\n",
    "    display(enriched_df.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Streaming Query\n",
    "\n",
    "This will run for 90 seconds, processing batches every 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "query = stream_df.writeStream.foreachBatch(process_batch).trigger(processingTime=\"30 seconds\").start()\n",
    "\n",
    "print(\"Streaming query started. Will run for 90 seconds...\")\n",
    "print(\"Watch for enriched batches below.\\n\")\n",
    "\n",
    "# Run for 90 seconds (3 batches)\n",
    "time.sleep(90)\n",
    "\n",
    "query.stop()\n",
    "print(\"\\nStreaming query stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Example: Write to Parquet\n",
    "\n",
    "In production, you'd write enriched results to a sink instead of displaying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Write to Parquet (uncomment to run)\n",
    "\n",
    "# def process_and_save(batch_df, batch_id):\n",
    "#     if batch_df.count() == 0:\n",
    "#         return\n",
    "#\n",
    "#     enriched_df = enrich_streaming_batch(\n",
    "#         batch_df,\n",
    "#         input_columns={\"company_name\": \"company_name\"},\n",
    "#         output_columns=[\"CEO name\", \"headquarters\"],\n",
    "#         processor=\"lite-fast\"\n",
    "#     )\n",
    "#\n",
    "#     enriched_df.write.mode(\"append\").parquet(\"/path/to/output\")\n",
    "\n",
    "print(\"See commented code above for production sink example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parallel-web-tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
