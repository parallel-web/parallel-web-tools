{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Data Enrichment with Apache Spark\n",
    "\n",
    "This notebook demonstrates how to use the `parallel-web-tools` package to enrich data directly in Apache Spark using SQL-native UDFs.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **SQL-native UDFs**: Use `parallel_enrich()` directly in Spark SQL queries\n",
    "- **DataFrame API**: Use UDFs with `withColumn()`, `select()`, etc.\n",
    "- **Multiple processors**: Choose speed vs. depth tradeoff (lite, base, core, pro, ultra)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install parallel-web-tools[spark]\n",
    "export PARALLEL_API_KEY=\"your-api-key\"\n",
    "```\n",
    "\n",
    "You also need Java Runtime Environment (JRE) for Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install parallel-web-tools[spark]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StringType, StructField, StructType\n",
    "\n",
    "# Import Parallel Spark integration\n",
    "from parallel_web_tools.integrations.spark import (\n",
    "    create_parallel_enrich_udf,\n",
    "    register_parallel_udfs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "# Note: spark.driver.bindAddress is set for local development compatibility\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .appName(\"ParallelEnrichmentDemo\")\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "\n",
    "The Spark UDFs use credentials in this order:\n",
    "1. `api_key` parameter passed to `register_parallel_udfs()`\n",
    "2. `PARALLEL_API_KEY` environment variable\n",
    "\n",
    "For detailed setup instructions for Databricks, EMR, Dataproc, and other platforms, see **[docs/spark-setup.md](../docs/spark-setup.md)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file (if present)\n",
    "# Create a .env file with: PARALLEL_API_KEY=your-key\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get(\"PARALLEL_API_KEY\")\n",
    "if api_key:\n",
    "    print(f\"PARALLEL_API_KEY is set ({len(api_key)} chars)\")\n",
    "else:\n",
    "    print(\"PARALLEL_API_KEY not found. Create a .env file with:\")\n",
    "    print(\"  PARALLEL_API_KEY=your-key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register UDFs with Spark\n",
    "\n",
    "This makes `parallel_enrich()` available in Spark SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register UDFs with default settings\n",
    "# processor=\"lite-fast\" is the default (fastest, cheapest)\n",
    "register_parallel_udfs(\n",
    "    spark,\n",
    "    processor=\"lite-fast\",  # Options: lite, base, core, pro, ultra (+ -fast variants)\n",
    "    timeout=300,  # Timeout in seconds\n",
    ")\n",
    "\n",
    "print(\"UDFs registered: parallel_enrich, parallel_enrich_with_processor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample company data\n",
    "companies = [\n",
    "    (\"Parallel Web Systems\", \"https://parallel.ai\", \"Technology\"),\n",
    "    (\"Google\", \"https://google.com\", \"Technology\"),\n",
    "    (\"Microsoft\", \"https://microsoft.com\", \"Technology\"),\n",
    "    (\"Apple\", \"https://apple.com\", \"Technology\"),\n",
    "    (\"Amazon\", \"https://amazon.com\", \"E-commerce\"),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(companies, [\"company_name\", \"website\", \"industry\"])\n",
    "\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register as temp view for SQL queries\n",
    "df.createOrReplaceTempView(\"companies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: SQL-based Enrichment\n",
    "\n",
    "Use `parallel_enrich()` directly in SQL queries. This is the most natural way to enrich data in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich companies with CEO name and founding year\n",
    "# Note: This will make API calls - may take a few seconds per row\n",
    "\n",
    "enriched_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        company_name,\n",
    "        website,\n",
    "        industry,\n",
    "        parallel_enrich(\n",
    "            map('company_name', company_name, 'website', website),\n",
    "            array(\n",
    "                'CEO name (current CEO or equivalent leader)',\n",
    "                'Founding year (YYYY format)',\n",
    "                'Brief company description (1-2 sentences)'\n",
    "            )\n",
    "        ) as enriched_json\n",
    "    FROM companies\n",
    "    LIMIT 2  -- Start with just 2 for demo\n",
    "\"\"\").cache()  # Cache to avoid re-running UDF on subsequent actions\n",
    "\n",
    "enriched_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the JSON Results\n",
    "\n",
    "The enrichment returns JSON. Let's parse it into structured columns.\n",
    "\n",
    "**Important**: We used `.cache()` above to store the enriched results. Without caching, Spark would re-execute the UDF (and make new API calls) each time we perform an action on derived DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for the enriched data\n",
    "enriched_schema = StructType(\n",
    "    [\n",
    "        StructField(\"ceo_name\", StringType(), True),\n",
    "        StructField(\"founding_year\", StringType(), True),\n",
    "        StructField(\"brief_company_description\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Parse JSON and extract fields (uses cached enriched_df - no re-computation)\n",
    "parsed_df = enriched_df.select(\n",
    "    \"company_name\", \"website\", \"industry\", from_json(col(\"enriched_json\"), enriched_schema).alias(\"enriched\")\n",
    ").select(\n",
    "    \"company_name\",\n",
    "    \"website\",\n",
    "    \"industry\",\n",
    "    col(\"enriched.ceo_name\").alias(\"ceo\"),\n",
    "    col(\"enriched.founding_year\").alias(\"founded\"),\n",
    "    col(\"enriched.brief_company_description\").alias(\"description\"),\n",
    ")\n",
    "\n",
    "parsed_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: DataFrame API with UDF\n",
    "\n",
    "Instead of SQL, you can use the UDF directly with Spark's DataFrame API (`withColumn`, `select`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, create_map, lit\n",
    "\n",
    "# Create a UDF instance (can customize processor, timeout, etc.)\n",
    "enrich_udf = create_parallel_enrich_udf(processor=\"lite-fast\", timeout=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the UDF with DataFrame API instead of SQL\n",
    "enriched_via_api = (\n",
    "    df.limit(2).withColumn(\n",
    "        \"enriched_json\",\n",
    "        enrich_udf(\n",
    "            # Input: map of column names to values\n",
    "            create_map(lit(\"company_name\"), col(\"company_name\"), lit(\"website\"), col(\"website\")),\n",
    "            # Output: array of field descriptions\n",
    "            array(lit(\"CEO name\"), lit(\"Founding year\"), lit(\"Brief company description\")),\n",
    "        ),\n",
    "    )\n",
    ").cache()\n",
    "\n",
    "enriched_via_api.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processor Options\n",
    "\n",
    "Choose a processor based on your needs:\n",
    "\n",
    "| Processor | Speed | Cost | Best For |\n",
    "|-----------|-------|------|----------|\n",
    "| `lite`, `lite-fast` | Fastest | Lowest | Basic metadata, high volume |\n",
    "| `base`, `base-fast` | Fast | Low | Standard enrichments |\n",
    "| `core`, `core-fast` | Medium | Medium | Cross-referenced data |\n",
    "| `pro`, `pro-fast` | Slow | High | Deep research (enables SSE) |\n",
    "| `ultra`, `ultra-fast` | Slowest | Highest | Multi-source research |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a different processor via SQL\n",
    "# parallel_enrich_with_processor allows per-query processor override\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        company_name,\n",
    "        parallel_enrich_with_processor(\n",
    "            map('company_name', company_name),\n",
    "            array('Recent news headline about this company'),\n",
    "            'base-fast'  -- Use base processor for more depth\n",
    "        ) as news\n",
    "    FROM companies\n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "result_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Enrichment\n",
    "\n",
    "For Spark Structured Streaming examples, see the dedicated notebook: **[spark_streaming_demo.ipynb](spark_streaming_demo.ipynb)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### 1. Always Cache Enriched Results\n",
    "The UDF makes API calls, so cache to avoid redundant requests:\n",
    "\n",
    "```python\n",
    "enriched_df = df.withColumn(\"enriched\", enrich_udf(...)).cache()\n",
    "# Now subsequent operations use cached data\n",
    "enriched_df.select(...).toPandas()\n",
    "```\n",
    "\n",
    "### 2. Column Descriptions\n",
    "Be specific in your output column descriptions:\n",
    "\n",
    "```python\n",
    "# Good - specific descriptions help the AI\n",
    "output_columns = array(\n",
    "    lit(\"CEO name (current CEO or equivalent leader)\"),\n",
    "    lit(\"Founding year (YYYY format)\"),\n",
    "    lit(\"Annual revenue (USD, most recent fiscal year)\")\n",
    ")\n",
    "\n",
    "# Less specific - may get inconsistent results\n",
    "output_columns = array(lit(\"CEO\"), lit(\"Year\"), lit(\"Revenue\"))\n",
    "```\n",
    "\n",
    "### 3. Error Handling\n",
    "Check for errors in the JSON response:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import get_json_object\n",
    "\n",
    "df.withColumn(\"error\", get_json_object(col(\"enriched_json\"), \"$.error\"))\n",
    "```\n",
    "\n",
    "### 4. Use Appropriate Processors\n",
    "- `lite-fast`: Basic metadata, high volume (cheapest)\n",
    "- `base-fast`: Standard enrichments\n",
    "- `pro-fast`: Deep research requiring multiple sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Production deployment**: Use `spark-submit` with your cluster\n",
    "- **Streaming**: Connect to Kafka, Kinesis, or file sources\n",
    "- **Scaling**: Increase parallelism with more executors\n",
    "- **Monitoring**: Track enrichment success/failure rates\n",
    "\n",
    "For more information:\n",
    "- [Parallel Documentation](https://docs.parallel.ai)\n",
    "- [parallel-web-tools GitHub](https://github.com/parallel-web/parallel-web-tools)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parallel-tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
