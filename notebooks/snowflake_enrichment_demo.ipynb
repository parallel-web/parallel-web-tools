{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Parallel Data Enrichment with Snowflake\n\nThis notebook demonstrates how to deploy and use the `parallel-web-tools` Snowflake integration for data enrichment.\n\n## Features\n\n- **Batched Table Function**: All rows processed in a single API call via `PARTITION BY`\n- **Secure API Access**: External Access Integration for secure HTTPS calls\n- **Multiple processors**: Choose speed vs. depth tradeoff\n- **Easy deployment**: Python helper or manual SQL\n\n## Prerequisites\n\n```bash\npip install parallel-web-tools[snowflake]\nexport PARALLEL_API_KEY=\"your-api-key\"\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install parallel-web-tools[snowflake]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from parallel_web_tools.integrations.snowflake import (\n",
    "    get_cleanup_sql,\n",
    "    get_setup_sql,\n",
    "    get_udf_sql,\n",
    ")\n",
    "\n",
    "print(\"Parallel Snowflake integration loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your Snowflake credentials and Parallel API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file (if present)\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get(\"PARALLEL_API_KEY\")\n",
    "if api_key:\n",
    "    print(f\"PARALLEL_API_KEY is set ({len(api_key)} chars)\")\n",
    "else:\n",
    "    print(\"PARALLEL_API_KEY not found. Create a .env file with:\")\n",
    "    print(\"  PARALLEL_API_KEY=your-key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Options\n",
    "\n",
    "### Option 1: Python Deployment (Recommended)\n",
    "\n",
    "Use the Python helper to deploy everything automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy to Snowflake (uncomment to run)\n",
    "# deploy_parallel_functions(\n",
    "#     account=SNOWFLAKE_ACCOUNT,\n",
    "#     user=SNOWFLAKE_USER,\n",
    "#     password=SNOWFLAKE_PASSWORD,\n",
    "#     parallel_api_key=os.environ.get(\"PARALLEL_API_KEY\"),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Manual SQL Deployment\n",
    "\n",
    "Get the SQL templates and run them manually in Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get setup SQL with your API key\n",
    "setup_sql = get_setup_sql(api_key=os.environ.get(\"PARALLEL_API_KEY\"))\n",
    "print(\"=\" * 60)\n",
    "print(\"SETUP SQL (01_setup.sql)\")\n",
    "print(\"=\" * 60)\n",
    "print(setup_sql[:2000] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get UDF creation SQL\n",
    "udf_sql = get_udf_sql()\n",
    "print(\"=\" * 60)\n",
    "print(\"UDF SQL (02_create_udf.sql)\")\n",
    "print(\"=\" * 60)\n",
    "print(udf_sql[:2000] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## SQL Usage Examples\n\nAfter deployment, you can use `parallel_enrich()` table function in Snowflake SQL.\n\n### Basic Enrichment\n\n```sql\nWITH companies AS (\n    SELECT * FROM (VALUES\n        ('Google', 'google.com'),\n        ('Anthropic', 'anthropic.com'),\n        ('Apple', 'apple.com')\n    ) AS t(company_name, website)\n)\nSELECT\n    e.input:company_name::STRING AS company_name,\n    e.input:website::STRING AS website,\n    e.enriched:ceo_name::STRING AS ceo_name,\n    e.enriched:founding_year::STRING AS founding_year\nFROM companies t,\n     TABLE(PARALLEL_INTEGRATION.ENRICHMENT.parallel_enrich(\n         TO_JSON(OBJECT_CONSTRUCT('company_name', t.company_name, 'website', t.website)),\n         ARRAY_CONSTRUCT('CEO name', 'Founding year')\n     ) OVER (PARTITION BY 1)) e;\n```\n\n**How it works:**\n- `PARTITION BY 1` batches all rows into a single API call\n- `TO_JSON(OBJECT_CONSTRUCT(...))` creates the input JSON\n- Returns `input` (original data) and `enriched` (results) columns"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Multiple Input Fields\n\n```sql\nSELECT\n    e.input:company_name::STRING AS company_name,\n    e.enriched:ceo_name::STRING AS ceo_name,\n    e.enriched:founding_year::STRING AS founding_year,\n    e.enriched:headquarters_city::STRING AS headquarters,\n    e.enriched:number_of_employees::STRING AS employees\nFROM my_companies t,\n     TABLE(PARALLEL_INTEGRATION.ENRICHMENT.parallel_enrich(\n         TO_JSON(OBJECT_CONSTRUCT(\n             'company_name', t.company_name,\n             'website', t.website,\n             'industry', t.industry\n         )),\n         ARRAY_CONSTRUCT(\n             'CEO name (current CEO or equivalent leader)',\n             'Founding year (YYYY format)',\n             'Headquarters city',\n             'Number of employees (approximate)'\n         )\n     ) OVER (PARTITION BY 1)) e;\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Custom Processor\n\nUse a different processor for more depth or faster results.\n\n```sql\nSELECT\n    e.input:company_name::STRING AS company_name,\n    e.enriched:ceo_name::STRING AS ceo_name,\n    e.enriched:recent_news_headline::STRING AS news\nFROM my_companies t,\n     TABLE(PARALLEL_INTEGRATION.ENRICHMENT.parallel_enrich(\n         TO_JSON(OBJECT_CONSTRUCT('company_name', t.company_name)),\n         ARRAY_CONSTRUCT('CEO name', 'Recent news headline'),\n         'base-fast'  -- processor option\n     ) OVER (PARTITION BY 1)) e;\n```\n\n**Processor Options:**\n\n| Processor | Speed | Cost | Best For |\n|-----------|-------|------|----------|\n| `lite-fast` | Fastest | Lowest | Basic metadata, high volume |\n| `base-fast` | Fast | Low | Standard enrichments |\n| `core-fast` | Medium | Medium | Cross-referenced data |\n| `pro-fast` | Slow | High | Deep research |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Enrich Table Rows\n\n```sql\n-- Enrich rows from an existing table\nSELECT\n    e.input:company_name::STRING AS company_name,\n    e.input:website::STRING AS website,\n    e.enriched:ceo_name::STRING AS ceo_name,\n    e.enriched:founding_year::STRING AS founding_year\nFROM companies t,\n     TABLE(PARALLEL_INTEGRATION.ENRICHMENT.parallel_enrich(\n         TO_JSON(OBJECT_CONSTRUCT('company_name', t.company_name, 'website', t.website)),\n         ARRAY_CONSTRUCT('CEO name', 'Founding year')\n     ) OVER (PARTITION BY 1)) e;\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Partitioning Strategies\n\nThe `PARTITION BY` clause controls how rows are batched into API calls.\n\n**All rows in one batch (default):**\n```sql\n-- Single API call for all rows\nTABLE(parallel_enrich(...) OVER (PARTITION BY 1))\n```\n\n**One batch per group:**\n```sql\n-- Process each region as a separate API call\nSELECT\n    e.input:company_name::STRING AS company_name,\n    e.input:region::STRING AS region,\n    e.enriched:ceo_name::STRING AS ceo_name\nFROM companies t,\n     TABLE(PARALLEL_INTEGRATION.ENRICHMENT.parallel_enrich(\n         TO_JSON(OBJECT_CONSTRUCT('company_name', t.company_name, 'region', t.region)),\n         ARRAY_CONSTRUCT('CEO name')\n     ) OVER (PARTITION BY t.region)) e;\n```\n\n**Fixed batch sizes:**\n```sql\n-- Process in batches of 100 rows each\nWITH numbered AS (\n    SELECT *, CEIL(ROW_NUMBER() OVER (ORDER BY company_name) / 100.0) AS batch_id\n    FROM companies\n)\nSELECT\n    e.input:company_name::STRING AS company_name,\n    e.enriched:ceo_name::STRING AS ceo_name\nFROM numbered t,\n     TABLE(PARALLEL_INTEGRATION.ENRICHMENT.parallel_enrich(\n         TO_JSON(OBJECT_CONSTRUCT('company_name', t.company_name)),\n         ARRAY_CONSTRUCT('CEO name')\n     ) OVER (PARTITION BY t.batch_id)) e;\n```\n\n| Pattern | Use Case |\n|---------|----------|\n| `PARTITION BY 1` | Small datasets, fastest for few rows |\n| `PARTITION BY column` | Natural groupings, incremental processing |\n| `PARTITION BY batch_id` | Large datasets with fixed batch sizes |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Save Results to Table\n\n```sql\n-- Create a new table with enriched data\nCREATE TABLE enriched_companies AS\nSELECT\n    e.input:company_name::STRING AS company_name,\n    e.input:website::STRING AS website,\n    e.enriched:ceo_name::STRING AS ceo_name,\n    e.enriched:founding_year::STRING AS founding_year\nFROM companies t,\n     TABLE(PARALLEL_INTEGRATION.ENRICHMENT.parallel_enrich(\n         TO_JSON(OBJECT_CONSTRUCT('company_name', t.company_name, 'website', t.website)),\n         ARRAY_CONSTRUCT('CEO name', 'Founding year')\n     ) OVER (PARTITION BY 1)) e;\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Name Mapping\n",
    "\n",
    "Output columns are automatically converted to valid JSON property names:\n",
    "\n",
    "| Description | Property Name |\n",
    "|-------------|---------------|\n",
    "| `\"CEO name\"` | `ceo_name` |\n",
    "| `\"Founding year (YYYY)\"` | `founding_year` |\n",
    "| `\"Annual revenue [USD]\"` | `annual_revenue` |\n",
    "| `\"2024 Revenue\"` | `col_2024_revenue` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Error Handling\n\nErrors are returned in the `enriched` column.\n\n```sql\n-- Check for errors in results\nSELECT\n    e.input:company_name::STRING AS company_name,\n    e.enriched:error::STRING AS error_message,\n    e.enriched:ceo_name::STRING AS ceo_name\nFROM companies t,\n     TABLE(PARALLEL_INTEGRATION.ENRICHMENT.parallel_enrich(\n         TO_JSON(OBJECT_CONSTRUCT('company_name', t.company_name)),\n         ARRAY_CONSTRUCT('CEO name')\n     ) OVER (PARTITION BY 1)) e;\n```\n\nCommon errors:\n- `\"No API key provided\"` - Secret not configured\n- `\"Timeout waiting for enrichment\"` - API took too long\n- `\"API request failed: ...\"` - Network or API error"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "To remove all Parallel integration objects from Snowflake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Python cleanup\n",
    "# cleanup_parallel_functions(\n",
    "#     account=SNOWFLAKE_ACCOUNT,\n",
    "#     user=SNOWFLAKE_USER,\n",
    "#     password=SNOWFLAKE_PASSWORD,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Get cleanup SQL\n",
    "cleanup_sql = get_cleanup_sql()\n",
    "print(\"=\" * 60)\n",
    "print(\"CLEANUP SQL (03_cleanup.sql)\")\n",
    "print(\"=\" * 60)\n",
    "print(cleanup_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Best Practices\n\n### 1. Use Specific Descriptions\n\n```sql\n-- Good - specific descriptions\nARRAY_CONSTRUCT(\n    'CEO name (current CEO or equivalent leader)',\n    'Founding year (YYYY format)',\n    'Annual revenue (USD, most recent fiscal year)'\n)\n\n-- Less specific - may get inconsistent results\nARRAY_CONSTRUCT('CEO', 'Year', 'Revenue')\n```\n\n### 2. Use Appropriate Processors\n\n- `lite-fast`: Basic metadata, high volume (cheapest, default)\n- `base-fast`: Standard company information\n- `core-fast`: Cross-referenced data from multiple sources\n- `pro-fast`: Deep research requiring multiple sources\n\n### 3. Batching via PARTITION BY\n\nUse `PARTITION BY 1` to batch all rows into a single API call:\n\n```sql\n-- All rows processed together (efficient)\nTABLE(parallel_enrich(...) OVER (PARTITION BY 1))\n\n-- Each group processed separately\nTABLE(parallel_enrich(...) OVER (PARTITION BY region))\n```\n\n### 4. Cache Results\n\n```sql\n-- Store enriched results to avoid re-processing\nCREATE TABLE enriched_cache AS\nSELECT e.input, e.enriched\nFROM source_table t,\n     TABLE(PARALLEL_INTEGRATION.ENRICHMENT.parallel_enrich(...) OVER (PARTITION BY 1)) e;\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Security\n",
    "\n",
    "The integration uses Snowflake's security features:\n",
    "\n",
    "1. **Network Rule**: Only allows egress to `api.parallel.ai:443`\n",
    "2. **Secret**: API key stored encrypted (not visible in SQL)\n",
    "3. **External Access Integration**: Combines rule and secret\n",
    "4. **Roles**: PARALLEL_DEVELOPER and PARALLEL_USER for permissions\n",
    "\n",
    "Grant PARALLEL_USER to users who need to run enrichments:\n",
    "\n",
    "```sql\n",
    "GRANT ROLE PARALLEL_USER TO USER analyst_user;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- See the [Snowflake Setup Guide](../docs/snowflake-setup.md) for more details\n",
    "- Check [Parallel Documentation](https://docs.parallel.ai) for API information\n",
    "- View [parallel-web-tools on GitHub](https://github.com/parallel-web/parallel-web-tools)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
