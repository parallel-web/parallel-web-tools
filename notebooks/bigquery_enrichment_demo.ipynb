{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Data Enrichment with BigQuery\n",
    "\n",
    "This notebook demonstrates how to use the `parallel-web-tools` package to enrich data directly in BigQuery using SQL-native Remote Functions.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "BigQuery SQL Query\n",
    "       │\n",
    "       ▼\n",
    "BigQuery Remote Function (parallel_enrich)\n",
    "       │\n",
    "       ▼\n",
    "Cloud Function (HTTP endpoint)\n",
    "       │\n",
    "       ▼\n",
    "Parallel Task API\n",
    "```\n",
    "\n",
    "## Features\n",
    "\n",
    "- **SQL-native**: Use `parallel_enrich()` directly in BigQuery SQL\n",
    "- **Serverless**: Cloud Function scales automatically\n",
    "- **Secure**: API key stored in Secret Manager\n",
    "- **Multiple processors**: Choose speed vs. depth tradeoff\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Google Cloud project with billing enabled\n",
    "2. `gcloud` CLI installed and authenticated\n",
    "3. `bq` CLI (comes with Google Cloud SDK)\n",
    "4. Parallel API key from [platform.parallel.ai](https://platform.parallel.ai)\n",
    "\n",
    "```bash\n",
    "# Install the package\n",
    "pip install parallel-web-tools\n",
    "\n",
    "# Authenticate with GCP\n",
    "gcloud auth login\n",
    "gcloud auth application-default login\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your GCP project and Parallel API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Your GCP project ID\n",
    "PROJECT_ID = \"your-gcp-project\"  # <-- CHANGE THIS\n",
    "REGION = \"us-central1\"\n",
    "DATASET_ID = \"parallel_functions\"\n",
    "\n",
    "# Your Parallel API key (or set PARALLEL_API_KEY env var)\n",
    "PARALLEL_API_KEY = os.environ.get(\"PARALLEL_API_KEY\", \"your-parallel-api-key\")  # <-- CHANGE THIS\n",
    "\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"API Key: {'*' * 8}...{PARALLEL_API_KEY[-4:] if len(PARALLEL_API_KEY) > 8 else '(not set)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Deploy the Integration\n",
    "\n",
    "Deploy the Cloud Function and BigQuery Remote Functions. This creates:\n",
    "\n",
    "1. **Secret** in Secret Manager for the API key\n",
    "2. **Cloud Function** (Gen2) that handles enrichment requests\n",
    "3. **BigQuery Connection** for remote function calls\n",
    "4. **BigQuery Dataset** (`parallel_functions`)\n",
    "5. **Remote Functions**:\n",
    "   - `parallel_enrich(input_data, output_columns)` - Main enrichment function\n",
    "   - `parallel_enrich_company(name, website, fields)` - Convenience function\n",
    "\n",
    "### Option A: Using Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parallel_web_tools.integrations.bigquery import deploy_bigquery_integration\n",
    "\n",
    "# Deploy the integration (takes 2-3 minutes)\n",
    "result = deploy_bigquery_integration(\n",
    "    project_id=PROJECT_ID,\n",
    "    api_key=PARALLEL_API_KEY,\n",
    "    region=REGION,\n",
    "    dataset_id=DATASET_ID,\n",
    ")\n",
    "\n",
    "print(f\"\\nFunction URL: {result['function_url']}\")\n",
    "print(f\"\\nExample query:\\n{result['example_query']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Using CLI\n",
    "\n",
    "Alternatively, deploy from the command line:\n",
    "\n",
    "```bash\n",
    "parallel-cli enrich deploy --system bigquery \\\n",
    "    --project=your-gcp-project \\\n",
    "    --region=us-central1 \\\n",
    "    --api-key=your-parallel-api-key\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Connect to BigQuery\n",
    "\n",
    "Now let's connect to BigQuery and run some enrichment queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Create BigQuery client\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "\n",
    "def run_query(sql):\n",
    "    \"\"\"Run a query and return results as a pandas DataFrame.\"\"\"\n",
    "    return client.query(sql).to_dataframe()\n",
    "\n",
    "\n",
    "print(f\"Connected to BigQuery project: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Basic Enrichment\n",
    "\n",
    "Use `parallel_enrich()` to enrich data directly in SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic enrichment - single company\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    'Google' as company_name,\n",
    "    `{PROJECT_ID}.{DATASET_ID}.parallel_enrich`(\n",
    "        JSON_OBJECT('company_name', 'Google', 'website', 'google.com'),\n",
    "        JSON_ARRAY('CEO name', 'Founding year', 'Brief description')\n",
    "    ) as enriched_data\n",
    "\"\"\"\n",
    "\n",
    "print(\"Running enrichment query...\")\n",
    "df = run_query(query)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Parse JSON Results\n",
    "\n",
    "The enrichment returns JSON. Use `JSON_EXTRACT_SCALAR` to parse into columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse JSON results into columns\n",
    "query = f\"\"\"\n",
    "WITH enriched AS (\n",
    "    SELECT\n",
    "        name,\n",
    "        `{PROJECT_ID}.{DATASET_ID}.parallel_enrich`(\n",
    "            JSON_OBJECT('company_name', name),\n",
    "            JSON_ARRAY(\n",
    "                'CEO name (current CEO)',\n",
    "                'Founding year (YYYY)',\n",
    "                'Headquarters city'\n",
    "            )\n",
    "        ) as info\n",
    "    FROM UNNEST(['Google', 'Microsoft', 'Apple']) as name\n",
    ")\n",
    "SELECT\n",
    "    name,\n",
    "    JSON_EXTRACT_SCALAR(info, '$.ceo_name') as ceo,\n",
    "    JSON_EXTRACT_SCALAR(info, '$.founding_year') as founded,\n",
    "    JSON_EXTRACT_SCALAR(info, '$.headquarters_city') as hq\n",
    "FROM enriched\n",
    "\"\"\"\n",
    "\n",
    "print(\"Running enrichment with JSON parsing...\")\n",
    "df = run_query(query)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Company Convenience Function\n",
    "\n",
    "Use `parallel_enrich_company()` for a simpler interface when enriching company data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience function for companies\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    `{PROJECT_ID}.{DATASET_ID}.parallel_enrich_company`(\n",
    "        'Parallel Web Systems',\n",
    "        'parallel.ai',\n",
    "        JSON_ARRAY('CEO name', 'Stock ticker', 'Number of employees')\n",
    "    ) as company_info\n",
    "\"\"\"\n",
    "\n",
    "print(\"Running company enrichment...\")\n",
    "df = run_query(query)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Enrich from Existing Tables\n",
    "\n",
    "Enrich data from your existing BigQuery tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a sample table\n",
    "create_table_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_ID}.sample_companies` AS\n",
    "SELECT * FROM UNNEST([\n",
    "    STRUCT('Amazon' as name, 'amazon.com' as website),\n",
    "    STRUCT('Netflix' as name, 'netflix.com' as website),\n",
    "    STRUCT('Spotify' as name, 'spotify.com' as website)\n",
    "])\n",
    "\"\"\"\n",
    "\n",
    "client.query(create_table_query).result()\n",
    "print(\"Created sample_companies table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich from the table\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    name,\n",
    "    website,\n",
    "    `{PROJECT_ID}.{DATASET_ID}.parallel_enrich`(\n",
    "        JSON_OBJECT('company_name', name, 'website', website),\n",
    "        JSON_ARRAY('Industry', 'Founded year', 'Business model')\n",
    "    ) as enriched_data\n",
    "FROM `{PROJECT_ID}.{DATASET_ID}.sample_companies`\n",
    "\"\"\"\n",
    "\n",
    "print(\"Enriching from table...\")\n",
    "df = run_query(query)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processor Options\n",
    "\n",
    "The default processor is `lite-fast`. To use a different processor, create a custom function with different settings.\n",
    "\n",
    "| Processor | Speed | Cost | Best For |\n",
    "|-----------|-------|------|----------|\n",
    "| `lite`, `lite-fast` | Fastest | ~$0.005/query | Basic metadata, high volume |\n",
    "| `base`, `base-fast` | Fast | ~$0.01/query | Standard enrichments |\n",
    "| `core`, `core-fast` | Medium | ~$0.025/query | Cross-referenced data |\n",
    "| `pro`, `pro-fast` | Slow | ~$0.10/query | Deep research |\n",
    "\n",
    "### Creating a Pro-tier Function\n",
    "\n",
    "```sql\n",
    "-- Create a function that uses pro-fast processor\n",
    "CREATE OR REPLACE FUNCTION `your-project.parallel_functions.parallel_enrich_pro`(\n",
    "    input_data STRING,\n",
    "    output_columns STRING\n",
    ")\n",
    "RETURNS STRING\n",
    "REMOTE WITH CONNECTION `your-project.us-central1.parallel-connection`\n",
    "OPTIONS (\n",
    "    endpoint = 'YOUR_FUNCTION_URL',\n",
    "    user_defined_context = [(\"processor\", \"pro-fast\")]\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### 1. Be Specific in Column Descriptions\n",
    "\n",
    "```sql\n",
    "-- Good - specific descriptions\n",
    "JSON_ARRAY(\n",
    "    'CEO name (current CEO or equivalent leader)',\n",
    "    'Founding year (YYYY format)',\n",
    "    'Annual revenue (USD, most recent fiscal year)'\n",
    ")\n",
    "\n",
    "-- Less specific - may get inconsistent results\n",
    "JSON_ARRAY('CEO', 'Year', 'Revenue')\n",
    "```\n",
    "\n",
    "### 2. Handle Errors\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    name,\n",
    "    CASE \n",
    "        WHEN JSON_EXTRACT_SCALAR(enriched, '$.error') IS NOT NULL\n",
    "        THEN CONCAT('Error: ', JSON_EXTRACT_SCALAR(enriched, '$.error'))\n",
    "        ELSE JSON_EXTRACT_SCALAR(enriched, '$.ceo_name')\n",
    "    END as ceo_or_error\n",
    "FROM ...\n",
    "```\n",
    "\n",
    "### 3. Batch Processing\n",
    "\n",
    "For large datasets, process in batches to avoid timeouts:\n",
    "\n",
    "```sql\n",
    "-- Process in batches of 100\n",
    "SELECT * FROM (\n",
    "    SELECT *, ROW_NUMBER() OVER() as rn\n",
    "    FROM your_table\n",
    ")\n",
    "WHERE rn BETWEEN 1 AND 100\n",
    "```\n",
    "\n",
    "### 4. Save Enriched Results\n",
    "\n",
    "```sql\n",
    "-- Create a table with enriched results\n",
    "CREATE TABLE `project.dataset.enriched_companies` AS\n",
    "SELECT\n",
    "    name,\n",
    "    parallel_enrich(...) as enriched_data,\n",
    "    CURRENT_TIMESTAMP() as enriched_at\n",
    "FROM source_table\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Estimation\n",
    "\n",
    "| Component | Cost |\n",
    "|-----------|------|\n",
    "| Cloud Functions | ~$0.40/million invocations + compute |\n",
    "| BigQuery | Query processing costs |\n",
    "| Parallel API | $0.005-$0.10 per enrichment |\n",
    "| Secret Manager | ~$0.06/10,000 accesses |\n",
    "\n",
    "**Example**: 1,000 company enrichments using `lite-fast`:\n",
    "- Parallel API: ~$5\n",
    "- GCP infrastructure: <$1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Deployment Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parallel_web_tools.integrations.bigquery import get_deployment_status\n",
    "\n",
    "status = get_deployment_status(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    ")\n",
    "\n",
    "if status[\"function_deployed\"]:\n",
    "    print(\"✓ Cloud Function deployed\")\n",
    "    print(f\"  URL: {status['function_url']}\")\n",
    "else:\n",
    "    print(\"✗ Cloud Function not deployed\")\n",
    "    print(\"  Run the deployment cell above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Remove all deployed resources when you're done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clean up all resources\n",
    "# WARNING: This will delete the Cloud Function, connection, dataset, and optionally the secret\n",
    "\n",
    "# from parallel_web_tools.integrations.bigquery import cleanup_bigquery_integration\n",
    "#\n",
    "# cleanup_bigquery_integration(\n",
    "#     project_id=PROJECT_ID,\n",
    "#     region=REGION,\n",
    "#     dataset_id=DATASET_ID,\n",
    "#     delete_secret=True,  # Also delete API key secret\n",
    "# )\n",
    "# print(\"Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or manually via gcloud:\n",
    "\n",
    "```bash\n",
    "gcloud functions delete parallel-enrich --gen2 --region=us-central1 --project=your-gcp-project --quiet\n",
    "bq rm --connection --force your-gcp-project.us-central1.parallel-connection\n",
    "bq rm -r -f your-gcp-project:parallel_functions\n",
    "gcloud secrets delete parallel-api-key --project=your-gcp-project --quiet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### \"Permission denied\" when calling function\n",
    "\n",
    "The BigQuery connection's service account needs Cloud Functions Invoker and Cloud Run Invoker roles:\n",
    "\n",
    "```bash\n",
    "# Get connection service account\n",
    "CONNECTION_SA=$(bq show --connection PROJECT.REGION.parallel-connection --format=json | jq -r '.cloudResource.serviceAccountId')\n",
    "\n",
    "# Grant Cloud Run Invoker (required for Gen2 functions)\n",
    "gcloud run services add-iam-policy-binding parallel-enrich \\\n",
    "    --region=us-central1 \\\n",
    "    --member=\"serviceAccount:$CONNECTION_SA\" \\\n",
    "    --role=\"roles/run.invoker\" \\\n",
    "    --project=your-project\n",
    "```\n",
    "\n",
    "### Function timeout\n",
    "\n",
    "- Use `lite-fast` processor for faster results\n",
    "- Process smaller batches\n",
    "- Increase function timeout (max 3600s for Gen2)\n",
    "\n",
    "### View logs\n",
    "\n",
    "```bash\n",
    "gcloud functions logs read parallel-enrich --gen2 --region=us-central1 --project=your-project\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Scheduled enrichment**: Use BigQuery scheduled queries to enrich new data periodically\n",
    "- **Data pipelines**: Integrate with Dataform, dbt, or Apache Airflow\n",
    "- **Custom processors**: Create additional functions with different processor tiers\n",
    "\n",
    "For more information:\n",
    "- [BigQuery Setup Guide](../docs/bigquery-setup.md)\n",
    "- [Parallel Documentation](https://docs.parallel.ai)\n",
    "- [parallel-web-tools GitHub](https://github.com/parallel-web/parallel-web-tools)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
